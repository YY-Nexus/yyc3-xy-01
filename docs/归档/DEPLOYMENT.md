# YYCÂ³ æ™ºèƒ½æ’æ‹”å¼ç§»åŠ¨AIç³»ç»Ÿ éƒ¨ç½²æŒ‡å—

## ğŸ“– æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨ä¸åŒç¯å¢ƒä¸­éƒ¨ç½²YYCÂ³æ™ºèƒ½æ’æ‹”å¼ç§»åŠ¨AIç³»ç»Ÿï¼ŒåŒ…æ‹¬å¼€å‘ç¯å¢ƒã€æµ‹è¯•ç¯å¢ƒå’Œç”Ÿäº§ç¯å¢ƒçš„éƒ¨ç½²æ–¹æ¡ˆã€‚

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### éƒ¨ç½²æ¶æ„å›¾

```mermaid
graph TB
    subgraph "è´Ÿè½½å‡è¡¡å±‚"
        LB[Nginxè´Ÿè½½å‡è¡¡å™¨]
        SSL[SSLç»ˆç«¯]
    end

    subgraph "åº”ç”¨å±‚"
        APP1[YYCÂ³åº”ç”¨å®ä¾‹1]
        APP2[YYCÂ³åº”ç”¨å®ä¾‹2]
        APP3[YYCÂ³åº”ç”¨å®ä¾‹3]
    end

    subgraph "æ•°æ®å±‚"
        PG[(PostgreSQLä¸»åº“)]
        PGR[(PostgreSQLä»åº“)]
        REDIS[(Redisé›†ç¾¤)]
        ES[(Elasticsearch)]
        VECTOR[(å‘é‡æ•°æ®åº“)]
    end

    subgraph "ç›‘æ§å±‚"
        PROM[Prometheus]
        GRAF[Grafana]
        JAEGER[Jaeger]
        KIBANA[Kibana]
    end

    Internet --> SSL
    SSL --> LB
    LB --> APP1
    LB --> APP2
    LB --> APP3

    APP1 --> PG
    APP1 --> REDIS
    APP1 --> ES
    APP1 --> VECTOR

    APP2 --> PG
    APP2 --> REDIS
    APP2 --> ES
    APP2 --> VECTOR

    APP3 --> PG
    APP3 --> REDIS
    APP3 --> ES
    APP3 --> VECTOR

    PROM --> APP1
    PROM --> APP2
    PROM --> APP3
```

### æœåŠ¡ç»„ä»¶

| æœåŠ¡ | ç«¯å£ | æè¿° | å‰¯ç½®æ¡ä»¶ |
|------|------|------|----------|
| **Nginx** | 80, 443 | åå‘ä»£ç†å’Œè´Ÿè½½å‡è¡¡ | - |
| **APIç½‘å…³** | 8080 | ä¸»åº”ç”¨å’ŒAPIæœåŠ¡ | æ•°æ®åº“ã€Redis |
| **å·¥å…·æœåŠ¡** | 3001 | AIå·¥å…·ç®¡ç†æœåŠ¡ | APIç½‘å…³ |
| **çŸ¥è¯†åº“æœåŠ¡** | 3002 | çŸ¥è¯†åº“å’Œå‘é‡æœç´¢æœåŠ¡ | APIç½‘å…³ã€å‘é‡DB |
| **PostgreSQL** | 5432 | ä¸»æ•°æ®åº“ | - |
| **Redis** | 6379 | ç¼“å­˜å’Œä¼šè¯å­˜å‚¨ | - |
| **Elasticsearch** | 9200 | å…¨æ–‡æœç´¢ | - |
| **Prometheus** | 9090 | æŒ‡æ ‡æ”¶é›† | - |
| **Grafana** | 3001 | ç›‘æ§é¢æ¿ | Prometheus |

## ğŸš€ å¿«é€Ÿéƒ¨ç½²

### ä¸€é”®éƒ¨ç½²è„šæœ¬

ä½¿ç”¨æä¾›çš„ä¸€é”®éƒ¨ç½²è„šæœ¬ï¼š

```bash
# å…‹éš†é¡¹ç›®
git clone https://github.com/yyc3/yyc3-xy-ai.git
cd yyc3-xy-ai

# é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env.local
# ç¼–è¾‘ .env.local æ–‡ä»¶ï¼Œå¡«å…¥å¿…è¦é…ç½®

# ä¸€é”®éƒ¨ç½²
chmod +x deploy.sh
./deploy.sh deploy
```

### Docker Composeéƒ¨ç½²

```bash
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f yyc3-main

# åœæ­¢æœåŠ¡
docker-compose down
```

## ğŸ”§ ç¯å¢ƒé…ç½®

### å¼€å‘ç¯å¢ƒ

#### ç³»ç»Ÿè¦æ±‚
- CPU: 2æ ¸å¿ƒä»¥ä¸Š
- å†…å­˜: 4GBä»¥ä¸Š
- å­˜å‚¨: 20GBä»¥ä¸Š
- æ“ä½œç³»ç»Ÿ: macOS, Linux, Windows (WSL2)

#### ç¯å¢ƒå˜é‡

```bash
# .env.development
NODE_ENV=development
PORT=8080
API_GATEWAY_PORT=8080
TOOL_SERVICE_PORT=3001
KNOWLEDGE_SERVICE_PORT=3002

# æ•°æ®åº“é…ç½®
DATABASE_URL=postgresql://postgres:password@localhost:5432/yyc3_ai_dev
REDIS_URL=redis://localhost:6379
ELASTICSEARCH_URL=http://localhost:9200

# AIæœåŠ¡é…ç½®
OPENAI_API_KEY=your-openai-api-key
ANTHROPIC_API_KEY=your-anthropic-api-key

# å¼€å‘é…ç½®
DEBUG_MODE=true
HOT_RELOAD=true
LOG_LEVEL=debug
```

#### å¯åŠ¨æ­¥éª¤

```bash
# 1. å¯åŠ¨æ•°æ®åº“æœåŠ¡
docker-compose up -d postgres redis elasticsearch

# 2. å®‰è£…ä¾èµ–
bun install

# 3. æ•°æ®åº“è¿ç§»
bun run migrate

# 4. å¯åŠ¨å¼€å‘æœåŠ¡å™¨
bun run dev
```

### æµ‹è¯•ç¯å¢ƒ

#### ç³»ç»Ÿè¦æ±‚
- CPU: 4æ ¸å¿ƒä»¥ä¸Š
- å†…å­˜: 8GBä»¥ä¸Š
- å­˜å‚¨: 50GBä»¥ä¸Š
- ç½‘ç»œ: 100Mbpsä»¥ä¸Š

#### ç¯å¢ƒå˜é‡

```bash
# .env.testing
NODE_ENV=testing
PORT=8080

# æ•°æ®åº“é…ç½®ï¼ˆç‹¬ç«‹æ•°æ®åº“ï¼‰
DATABASE_URL=postgresql://postgres:password@postgres-test:5432/yyc3_ai_test
REDIS_URL=redis://redis-test:6379

# AIæœåŠ¡é…ç½®ï¼ˆä½¿ç”¨æµ‹è¯•APIï¼‰
OPENAI_API_KEY=your-test-openai-key
ANTHROPIC_API_KEY=your-test-anthropic-key

# æµ‹è¯•é…ç½®
TEST_MODE=true
MOCK_EXTERNAL_APIS=true
```

#### éƒ¨ç½²é…ç½®

```yaml
# docker-compose.test.yml
version: '3.8'

services:
  yyc3-test:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    environment:
      - NODE_ENV=testing
      - DATABASE_URL=postgresql://postgres:password@postgres-test:5432/yyc3_ai_test
    depends_on:
      - postgres-test
      - redis-test
    networks:
      - test-network

  postgres-test:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=yyc3_ai_test
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres-test-data:/var/lib/postgresql/data
    networks:
      - test-network

  redis-test:
    image: redis:7-alpine
    networks:
      - test-network

networks:
  test-network:
    driver: bridge

volumes:
  postgres-test-data:
```

### ç”Ÿäº§ç¯å¢ƒ

#### ç³»ç»Ÿè¦æ±‚

**æœ€å°é…ç½®**:
- CPU: 8æ ¸å¿ƒ
- å†…å­˜: 16GB
- å­˜å‚¨: 200GB SSD
- ç½‘ç»œ: 1Gbps

**æ¨èé…ç½®**:
- CPU: 16æ ¸å¿ƒ
- å†…å­˜: 32GB
- å­˜å‚¨: 500GB SSD
- ç½‘ç»œ: 10Gbps
- é«˜å¯ç”¨æ¶æ„ï¼ˆå¤šèŠ‚ç‚¹ï¼‰

#### ç¯å¢ƒå˜é‡

```bash
# .env.production
NODE_ENV=production
PORT=8080

# å®‰å…¨é…ç½®
JWT_SECRET=your-super-secure-jwt-secret-key
SESSION_SECRET=your-super-secure-session-secret
CORS_ORIGIN=https://yourdomain.com

# æ•°æ®åº“é…ç½®ï¼ˆä¸»ä»ï¼‰
DATABASE_URL=postgresql://user:password@postgres-master:5432/yyc3_ai
DATABASE_READ_URL=postgresql://user:password@postgres-slave:5432/yyc3_ai
REDIS_URL=redis://redis-cluster:6379
REDIS_SENTINEL_URL=redis://redis-sentinel:26379

# AIæœåŠ¡é…ç½®ï¼ˆç”Ÿäº§APIå¯†é’¥ï¼‰
OPENAI_API_KEY=sk-prod-xxxxxxxxxxxxxxxxxxxx
ANTHROPIC_API_KEY=sk-ant-prod-xxxxxxxxxxxxxxxxxxxx

# ç›‘æ§é…ç½®
PROMETHEUS_ENABLED=true
JAEGER_ENABLED=true
SENTRY_DSN=https://your-sentry-dsn

# æ€§èƒ½é…ç½®
ENABLE_CACHING=true
ENABLE_COMPRESSION=true
RATE_LIMIT_ENABLED=true
```

#### ç”Ÿäº§éƒ¨ç½²é…ç½®

```yaml
# docker-compose.prod.yml
version: '3.8'

services:
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./config/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./config/nginx/ssl:/etc/nginx/ssl:ro
      - ./logs/nginx:/var/log/nginx
    depends_on:
      - yyc3-app
    restart: unless-stopped
    networks:
      - yyc3-network

  yyc3-app:
    build:
      context: .
      dockerfile: Dockerfile
      target: production
    environment:
      - NODE_ENV=production
    env_file:
      - .env.production
    volumes:
      - ./logs/app:/app/logs
      - ./uploads:/app/uploads
    deploy:
      replicas: 3
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '1.0'
          memory: 1G
    depends_on:
      postgres-master:
        condition: service_healthy
      redis-cluster:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - yyc3-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  postgres-master:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=yyc3_ai
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_REPLICATION_MODE=master
      - POSTGRES_REPLICATION_USER=replicator
      - POSTGRES_REPLICATION_PASSWORD=${REPLICATION_PASSWORD}
    volumes:
      - postgres-master-data:/var/lib/postgresql/data
      - ./config/postgres/postgresql.conf:/etc/postgresql/postgresql.conf
      - ./config/postgres/pg_hba.conf:/etc/postgresql/pg_hba.conf
    ports:
      - "5432:5432"
    restart: unless-stopped
    networks:
      - yyc3-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5

  postgres-slave:
    image: postgres:15-alpine
    environment:
      - PGUSER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_MASTER_HOST=postgres-master
      - POSTGRES_REPLICATION_USER=replicator
      - POSTGRES_REPLICATION_PASSWORD=${REPLICATION_PASSWORD}
    volumes:
      - postgres-slave-data:/var/lib/postgresql/data
    depends_on:
      - postgres-master
    restart: unless-stopped
    networks:
      - yyc3-network

  redis-cluster:
    image: redis:7-alpine
    command: redis-server --appendonly yes --cluster-enabled yes
    volumes:
      - redis-data:/data
    ports:
      - "6379:6379"
    restart: unless-stopped
    networks:
      - yyc3-network
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - node.name=elasticsearch
      - cluster.name=yyc3-cluster
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms2g -Xmx2g"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    ports:
      - "9200:9200"
      - "9300:9300"
    restart: unless-stopped
    networks:
      - yyc3-network

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - yyc3-network

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana-data:/var/lib/grafana
      - ./config/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./config/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    depends_on:
      - prometheus
    restart: unless-stopped
    networks:
      - yyc3-network

networks:
  yyc3-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

volumes:
  postgres-master-data:
  postgres-slave-data:
  redis-data:
  elasticsearch-data:
  prometheus-data:
  grafana-data:
```

## ğŸ”’ å®‰å…¨é…ç½®

### SSL/TLSé…ç½®

```nginx
# config/nginx/nginx.conf
server {
    listen 443 ssl http2;
    server_name yourdomain.com;

    ssl_certificate /etc/nginx/ssl/cert.pem;
    ssl_certificate_key /etc/nginx/ssl/key.pem;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-RSA-AES256-GCM-SHA512:DHE-RSA-AES256-GCM-SHA512;
    ssl_prefer_server_ciphers off;

    location / {
        proxy_pass http://yyc3-app:8080;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }
}

server {
    listen 80;
    server_name yourdomain.com;
    return 301 https://$server_name$request_uri;
}
```

### é˜²ç«å¢™é…ç½®

```bash
# Ubuntu/Debian
sudo ufw enable
sudo ufw allow 22/tcp    # SSH
sudo ufw allow 80/tcp    # HTTP
sudo ufw allow 443/tcp   # HTTPS
sudo ufw deny 5432/tcp   # æ•°æ®åº“ï¼ˆä»…å†…éƒ¨è®¿é—®ï¼‰
sudo ufw deny 6379/tcp   # Redisï¼ˆä»…å†…éƒ¨è®¿é—®ï¼‰
sudo ufw deny 9200/tcp   # Elasticsearchï¼ˆä»…å†…éƒ¨è®¿é—®ï¼‰

# CentOS/RHEL
sudo firewall-cmd --permanent --add-service=ssh
sudo firewall-cmd --permanent --add-service=http
sudo firewall-cmd --permanent --add-service=https
sudo firewall-cmd --reload
```

### å¯†é’¥ç®¡ç†

```bash
# ç”ŸæˆJWTå¯†é’¥
openssl genrsa -out jwt-private.pem 4096
openssl rsa -in jwt-private.pem -pubout -out jwt-public.pem

# ç”ŸæˆSSLè¯ä¹¦ï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨Let's Encryptï¼‰
openssl req -x509 -nodes -days 365 -newkey rsa:2048 \
  -keyout ssl/private.key \
  -out ssl/certificate.crt \
  -subj "/C=CN/ST=State/L=City/O=Organization/CN=yourdomain.com"
```

## ğŸ“Š ç›‘æ§å’Œæ—¥å¿—

### Prometheusé…ç½®

```yaml
# config/prometheus/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'yyc3-app'
    static_configs:
      - targets: ['yyc3-app:8080']
    metrics_path: '/api/metrics'
    scrape_interval: 10s

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-master:5432']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-cluster:6379']

  - job_name: 'elasticsearch'
    static_configs:
      - targets: ['elasticsearch:9200']

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

### Grafanaä»ªè¡¨æ¿

```json
{
  "dashboard": {
    "title": "YYCÂ³ç³»ç»Ÿç›‘æ§",
    "panels": [
      {
        "title": "APIè¯·æ±‚é‡",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(http_requests_total[5m])",
            "legendFormat": "{{method}} {{endpoint}}"
          }
        ]
      },
      {
        "title": "å“åº”æ—¶é—´",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          }
        ]
      },
      {
        "title": "é”™è¯¯ç‡",
        "type": "singlestat",
        "targets": [
          {
            "expr": "rate(http_requests_total{status=~\"5..\"}[5m]) / rate(http_requests_total[5m]) * 100"
          }
        ]
      }
    ]
  }
}
```

### æ—¥å¿—é…ç½®

```typescript
// utils/logger.ts
import winston from 'winston';

const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.json()
  ),
  defaultMeta: { service: 'yyc3-ai' },
  transports: [
    new winston.transports.File({
      filename: 'logs/error.log',
      level: 'error',
      maxsize: 10 * 1024 * 1024, // 10MB
      maxFiles: 5
    }),
    new winston.transports.File({
      filename: 'logs/combined.log',
      maxsize: 10 * 1024 * 1024,
      maxFiles: 5
    }),
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.simple()
      )
    })
  ]
});

if (process.env.NODE_ENV === 'production') {
  logger.add(new winston.transports.File({
    filename: 'logs/audit.log',
    level: 'info',
    maxsize: 50 * 1024 * 1024,
    maxFiles: 10
  }));
}

export default logger;
```

## ğŸ”„ å¤‡ä»½å’Œæ¢å¤

### æ•°æ®åº“å¤‡ä»½

```bash
#!/bin/bash
# scripts/backup-database.sh

BACKUP_DIR="/backups"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="$BACKUP_DIR/yyc3_ai_backup_$DATE.sql"

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p $BACKUP_DIR

# æ‰§è¡Œå¤‡ä»½
docker-compose exec postgres-master pg_dump -U postgres -d yyc3_ai > $BACKUP_FILE

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
gzip $BACKUP_FILE

# åˆ é™¤7å¤©å‰çš„å¤‡ä»½
find $BACKUP_DIR -name "*.sql.gz" -mtime +7 -delete

echo "å¤‡ä»½å®Œæˆ: $BACKUP_FILE.gz"
```

### è‡ªåŠ¨å¤‡ä»½é…ç½®

```bash
# æ·»åŠ åˆ°crontab
# æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œå¤‡ä»½
0 2 * * * /path/to/backup-database.sh

# æ¯å‘¨æ—¥å‡Œæ™¨3ç‚¹æ‰§è¡Œå…¨é‡å¤‡ä»½
0 3 * * 0 /path/to/full-backup.sh
```

### æ•°æ®æ¢å¤

```bash
#!/bin/bash
# scripts/restore-database.sh

BACKUP_FILE=$1

if [ -z "$BACKUP_FILE" ]; then
    echo "ç”¨æ³•: $0 <backup_file>"
    exit 1
fi

# åœæ­¢åº”ç”¨æœåŠ¡
docker-compose stop yyc3-app

# æ¢å¤æ•°æ®åº“
gunzip -c $BACKUP_FILE | docker-compose exec -T postgres-master psql -U postgres -d yyc3_ai

# é‡å¯åº”ç”¨æœåŠ¡
docker-compose start yyc3-app

echo "æ•°æ®åº“æ¢å¤å®Œæˆ"
```

## ğŸš€ CI/CDéƒ¨ç½²

### GitHub Actionsé…ç½®

```yaml
# .github/workflows/deploy.yml
name: Deploy YYCÂ³ AI System

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          bun-version: latest

      - name: Install dependencies
        run: bun install

      - name: Run type checking
        run: bun run type-check

      - name: Run linting
        run: bun run lint

      - name: Run tests
        run: bun run test

  build:
    needs: test
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
      - uses: actions/checkout@v3

      - name: Setup Bun
        uses: oven-sh/setup-bun@v1
        with:
          bun-version: latest

      - name: Install dependencies
        run: bun install

      - name: Build application
        run: bun run build

      - name: Build Docker image
        run: |
          docker build -t yyc3/xy-ai:${{ github.sha }} .
          docker tag yyc3/xy-ai:latest yyc3/xy-ai:${{ github.sha }}

      - name: Push to registry
        run: |
          echo ${{ secrets.DOCKER_PASSWORD }} | docker login -u ${{ secrets.DOCKER_USERNAME }} --password-stdin
          docker push yyc3/xy-ai:${{ github.sha }}
          docker push yyc3/xy-ai:latest

  deploy:
    needs: build
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'

    steps:
      - name: Deploy to production
        uses: appleboy/ssh-action@v0.1.4
        with:
          host: ${{ secrets.PROD_HOST }}
          username: ${{ secrets.PROD_USER }}
          key: ${{ secrets.PROD_KEY }}
          script: |
            cd /opt/yyc3-xy-ai
            git pull origin main
            docker-compose pull
            docker-compose up -d
            docker system prune -f
```

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### 1. æœåŠ¡æ— æ³•å¯åŠ¨

```bash
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
docker-compose logs yyc3-app

# æ£€æŸ¥ç«¯å£å ç”¨
netstat -tlnp | grep :8080

# æ£€æŸ¥ç£ç›˜ç©ºé—´
df -h

# æ£€æŸ¥å†…å­˜ä½¿ç”¨
free -m
```

#### 2. æ•°æ®åº“è¿æ¥å¤±è´¥

```bash
# æ£€æŸ¥æ•°æ®åº“çŠ¶æ€
docker-compose exec postgres-master pg_isready -U postgres

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
docker-compose exec yyc3-app curl -f http://postgres-master:5432

# æ£€æŸ¥ç½‘ç»œè¿é€šæ€§
docker network ls
docker network inspect yyc3_xy-ai_yyc3-network
```

#### 3. AIæœåŠ¡æ— å“åº”

```bash
# æ£€æŸ¥APIå¯†é’¥
grep OPENAI_API_KEY .env.production

# æµ‹è¯•APIè¿æ¥
curl -H "Authorization: Bearer $OPENAI_API_KEY" \
     https://api.openai.com/v1/models

# æ£€æŸ¥ç½‘ç»œè¿æ¥
ping api.openai.com
```

### æ€§èƒ½ä¼˜åŒ–

#### æ•°æ®åº“ä¼˜åŒ–

```sql
-- PostgreSQLé…ç½®ä¼˜åŒ–
-- postgresql.conf
shared_buffers = 256MB
effective_cache_size = 1GB
maintenance_work_mem = 64MB
checkpoint_completion_target = 0.9
wal_buffers = 16MB
default_statistics_target = 100

-- ç´¢å¼•ä¼˜åŒ–
CREATE INDEX CONCURRENTLY idx_ai_messages_created_at ON ai_messages(created_at);
CREATE INDEX CONCURRENTLY idx_goals_status_priority ON goals(status, priority);
CREATE INDEX CONCURRENTLY idx_knowledge_vectors_vector_id ON knowledge_vectors(vector_id);
```

#### Redisä¼˜åŒ–

```conf
# redis.conf
maxmemory 2gb
maxmemory-policy allkeys-lru
save 900 1
save 300 10
save 60 10000
```

#### åº”ç”¨ä¼˜åŒ–

```typescript
// è¿æ¥æ± é…ç½®
const pool = new Pool({
  host: 'localhost',
  port: 5432,
  database: 'yyc3_ai',
  user: 'postgres',
  password: password,
  max: 20,
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

// ç¼“å­˜é…ç½®
const cacheOptions = {
  max: 1000,
  ttl: 1000 * 60 * 5, // 5åˆ†é’Ÿ
  updateAgeOnGet: true,
  allowStale: false,
};
```

## ğŸ“‹ éƒ¨ç½²æ¸…å•

### éƒ¨ç½²å‰æ£€æŸ¥

- [ ] æœåŠ¡å™¨èµ„æºæ»¡è¶³æœ€ä½è¦æ±‚
- [ ] ç½‘ç»œé…ç½®å’Œé˜²ç«å¢™è§„åˆ™
- [ ] SSLè¯ä¹¦å‡†å¤‡
- [ ] æ•°æ®åº“è®¾è®¡å’Œè¿ç§»è„šæœ¬
- [ ] ç¯å¢ƒå˜é‡é…ç½®
- [ ] ç›‘æ§å’Œæ—¥å¿—é…ç½®
- [ ] å¤‡ä»½ç­–ç•¥åˆ¶å®š
- [ ] å›æ»šæ–¹æ¡ˆå‡†å¤‡

### éƒ¨ç½²åéªŒè¯

- [ ] æ‰€æœ‰æœåŠ¡æ­£å¸¸å¯åŠ¨
- [ ] å¥åº·æ£€æŸ¥æ¥å£æ­£å¸¸
- [ ] æ•°æ®åº“è¿æ¥æ­£å¸¸
- [ ] ç¼“å­˜æœåŠ¡æ­£å¸¸
- [ ] AIæœåŠ¡å“åº”æ­£å¸¸
- [ ] ç›‘æ§æŒ‡æ ‡æ­£å¸¸
- [ ] æ—¥å¿—æ”¶é›†æ­£å¸¸
- [ ] æ€§èƒ½æµ‹è¯•é€šè¿‡

### éƒ¨ç½²æ–‡æ¡£

- [ ] ç³»ç»Ÿæ¶æ„æ–‡æ¡£
- [ ] APIæ¥å£æ–‡æ¡£
- [ ] è¿ç»´æ‰‹å†Œ
- [ ] æ•…éšœæ’é™¤æŒ‡å—
- [ ] åº”æ€¥å“åº”æµç¨‹

---

æœ€åæ›´æ–°: 2024-01-01
---

<div align="center">

> ã€Œ***YanYuCloudCube***ã€
> ã€Œ***<admin@0379.email>***ã€
> ã€Œ***Words Initiate Quadrants, Language Serves as Core for the Future***ã€
> ã€Œ***All things converge in the cloud pivot; Deep stacks ignite a new era of intelligence***ã€

</div>

